{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4da12f1-2b6a-4d18-a752-fe53170c4ca6",
   "metadata": {},
   "source": [
    "## Diffusion Model for FastMRI - Training\n",
    "\n",
    "This code is for training a denoising diffusion probabilistic model (DDPM) to conditionally generate brain MR images of different contrasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3282cb7b-dfc2-4119-865f-030e40ffbd20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fastmri_brain_data_builder import DataGenImagesOnly, DataGenImagesDownsampled\n",
    "from unet import real_to_complex, complex_to_real, ContextUnet, BigContextUnet\n",
    "from ddpm import DDPM\n",
    "\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d865b9-dd78-4dee-9d38-22179d969cc9",
   "metadata": {},
   "source": [
    "#### Initialize training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a028c9-8d66-4b10-af2e-3921f87187d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_epoch = 500\n",
    "batch_sz = 8\n",
    "num_sub = 20      # number of subjects of each contrast\n",
    "res = 160         # resolution after downsampling\n",
    "n_T = 500         # noise levels\n",
    "complex_in = False\n",
    "num_feat = 64\n",
    "\n",
    "load_save = True\n",
    "load_save_dir = './cond_ddpm_models/May15_1510_mag/'\n",
    "chkpoint = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5c1da9-7047-4256-9b2f-7634e0be9712",
   "metadata": {},
   "source": [
    "#### Load dataset\n",
    "\n",
    "We will be using FastMRI multicoil brain data, which contains images of four different contrasts: FLAIR, T1POST, T1, and T2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2458b59f-b0ea-484f-90c9-80b4c1b9ea41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = '../LSS/lss_jcb/aniket/FastMRI brain data/'\n",
    "ds = DataGenImagesDownsampled(start_sub=0, num_sub=num_sub, device=device, res=res, complex_in=complex_in, data_path=data_path)\n",
    "\n",
    "num_classes = len(ds.labels_key)\n",
    "num_slices = len(ds)\n",
    "idx = 0\n",
    "img_shape = ds[idx][0].shape\n",
    "print(\"Num images:\", num_slices)\n",
    "print(f\"Image {idx} shape:\", ds[idx][0].shape)\n",
    "print(f\"Image {idx} min/max:\", ds[idx][0].abs().min(), \"/\", ds[idx][0].abs().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af83abf-5d22-4690-b00b-8c96debd0b23",
   "metadata": {},
   "source": [
    "### Setup before training\n",
    "\n",
    "#### Set up save directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99add721-3972-4f07-9f32-8fd6a9c6e456",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now()\n",
    "using_mag = \"comp\" if complex_in else \"mag\"\n",
    "save_dir = f\"./cond_ddpm_models/{timestamp.strftime('%b%d_%H%M')}_{using_mag}\"\n",
    "if not(os.path.exists(save_dir)):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "print(f'\\nSaving to {save_dir}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa9b66f-8a36-4a4d-abd8-21968ba83f9c",
   "metadata": {},
   "source": [
    "#### Initialize score matching network (e.g. a U-net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2850e1-d109-4794-9089-16b1213cb04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps_model = ContextUnet(complex_in=complex_in, n_classes=num_classes)\n",
    "eps_model = ContextUnet(complex_in=complex_in, n_feat=num_feat, n_classes=num_classes)\n",
    "\n",
    "# Helper functions to log progress and save model weights\n",
    "def log(s, mute=True):\n",
    "    with open(save_dir+'/log.txt', 'a', encoding='utf8') as f: f.write(s+\"\\n\")\n",
    "    if not mute: print(s)\n",
    "        \n",
    "def save_checkpoint(states, path, filename='model_best.pth.tar'):\n",
    "    checkpoint_name = os.path.join(path, filename)\n",
    "    torch.save(states, checkpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca27e13-1b6a-43e1-bb94-b45dcb3b2c34",
   "metadata": {},
   "source": [
    "#### Initialize diffusion model with score matching network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba68b01-79ce-4cdf-b2d8-2134c2015303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize \n",
    "ddpm = DDPM(eps_model=eps_model, betas=(1e-4, 0.02), n_T=n_T, complex_in=complex_in, device=device)\n",
    "ddpm.to(device)\n",
    "\n",
    "if load_save:    \n",
    "    weights_fname = load_save_dir + f'/ddpm_ep{chkpoint}.pth'\n",
    "    ddpm.load_state_dict(torch.load(weights_fname))\n",
    "    log(f\"Loaded saved weights from {weights_fname}\", False)\n",
    "\n",
    "optim = torch.optim.Adam(ddpm.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1cea9e-d6d6-4f2e-a31f-f4e9255f500b",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799cd602-fcf7-48ca-a82d-c03658781293",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(ds, batch_size=batch_sz, shuffle=True, num_workers=1)\n",
    "\n",
    "with tqdm(total=n_epoch, position=0, leave=True) as pbar:\n",
    "    for i in range(1, n_epoch+1):\n",
    "        ddpm.train()\n",
    "        loss_ema = None\n",
    "        \n",
    "        for x, label in dataloader:\n",
    "            \n",
    "            x, label = x.to(device), label.to(device)\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            x = x.to(device)\n",
    "            loss = ddpm(x, label)\n",
    "            loss.backward()\n",
    "            if loss_ema is None:\n",
    "                loss_ema = loss.item()\n",
    "            else:\n",
    "                loss_ema = 0.9 * loss_ema + 0.1 * loss.item()\n",
    "            \n",
    "            optim.step()\n",
    "        \n",
    "        summary = f\"[Epoch {i}]\\tloss: {loss_ema:.6f}\"\n",
    "        pbar.set_description(summary)\n",
    "        pbar.update(1)\n",
    "            \n",
    "        if i == 1 or i % 50 == 0:\n",
    "            log(summary)\n",
    "                \n",
    "            ddpm.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                xh = ddpm.sample(batch_sz, (1, img_shape[-2], img_shape[-1]))\n",
    "                if complex_in: xh = xh.abs()\n",
    "                grid = make_grid(xh, nrow=4)\n",
    "                save_image(grid, save_dir+f\"/ddpm_sample_{i:03d}.png\")\n",
    "\n",
    "                # save model\n",
    "                torch.save(ddpm.state_dict(), save_dir+f\"/ddpm_ep{i}.pth\")\n",
    "                \n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45154984-4868-4b49-b263-fcd292b7b214",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "\n",
    "#### Generate samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd27e08d-08b2-4e76-8410-6d06f725cbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpm.eval()\n",
    "with torch.no_grad():\n",
    "    out_samples_list, nT_list = ddpm.sample_all((1, img_shape[-2], img_shape[-1]))\n",
    "    out_samples = out_samples_list[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0269c04b-30ea-46f0-88ff-0cedec891843",
   "metadata": {},
   "source": [
    "#### Display images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baaeda3-5fc2-4fa1-bb25-257c0f943259",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, num_classes, figsize=(20,10))\n",
    "plt.subplots_adjust(left=0, right=1, top=1, bottom=0, wspace=0.01)\n",
    "for i in range(num_classes):\n",
    "    img = out_samples_list[-1][i,0]\n",
    "    if complex_in: img = np.abs(img)   \n",
    "    # img = np.abs(img)   \n",
    "    ax[i].imshow(img, cmap='gray', vmin=0., vmax=1.)\n",
    "    ax[i].axis('off')\n",
    "    ax[i].set_title(f'{ds.labels_key[i]} sample')\n",
    "fig.suptitle('Final samples')\n",
    "    \n",
    "plt.savefig(save_dir+f'/sample_all_contrasts_{chkpoint}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed110de-d4ba-4833-9297-3a0609fda6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
